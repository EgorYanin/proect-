---
title: "МСМ_Компьютерная№2_12гр"
author: "Группа_12"
date: "2025-05-05"
output: html_document
editor_options: 
  markdown: 
    wrap: sentence
---

## 0. Подготовка к работе.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Выгрузим необходимые для работы библиотеки:

```{r}
library(knitr)
library(kableExtra)
library(magrittr)
library(openxlsx)
library(pander)
library(corrplot)
library(caret)
library(FactoMineR)
library(factoextra)
library(devtools)
library(rio)
library(corrplot)
library(psych)
library(ggpubr)
library(REdaS)
library(ggplot2)
library(tidyr)
library(dplyr)
library(broom)
library(GGally)
library(MASS)
```

Откроем файл и выгрузим проектные данные:

```{r}
df <- read.csv('coffee_shop_revenue.csv')
df <- subset(df, select = -Operating_Hours_Per_Day)
```

Удалим зависимую переменную (Daily_Revenue):

```{r}
df1 <- df[, 1:5]
```

И шкалируем данные для корректного применения методов анализа и кластеризации:

```{r}
df_s <- scale(df1)
```

## 1. Выделение главных компонент.

### 1.1 Выделение главных компонент.

Для начала проверим применимость метода МГК путем проведения теста сферичности Бартлетта:

```{r}
bart_spher(df_s)
```

Значение p-value значимо больше 0,05, следовательно, признаки не коррелированы между собой.
Предпосылки для применения МГК нет, однако в рамках нашего задания мы все равно применим его для наглядности его несостоятельности.

Для начала определим главные компоненты:

```{r}
princomp <- princomp(df_s, cor = TRUE)
summary(princomp)
```

Как видно, три первые компоненты имеют значения собственных векторов выше 1, доли

1)  Метод Кайзера:

```{r}
pc <- PCA(df_s, graph = FALSE)
pc$eig
```

Мы видим, что действительно только у первых трех компонент собственное значение вектора превосходит 1.

2)  Доля суммарной вариации:

```{r}
pc$eig
```

Как и было описано ранее, тут 4-ая компонента имеет больший накопительный вклад и т.к накопительный вклад третьей меньше 70%, нужно взять четвертую компоненту.

3)  Критерий каменистой осыпи.

```{r}
fviz_eig(pc, addlabels = TRUE)
```

Согласно графику, переход от 3 компоненте к 4-ой довольно резкий.
Поэтому в учет берем только три компоненты.
Так как два метода показали, что отобрать стоит три первые компоненты, отберем именно столько.

### 1.2 Описание суммарного вклада первых компонент.

Вновь возвращаемся к таблице:

```{r}
princomp <- princomp(df_s, cor = TRUE)
summary(princomp)
```

В таблице наглядно виден вклад компонент, так первая главная компонента объясняет 21,19% дисперсии, вторая - 20,41%, третья - 19,12%.
Всего вместе они объясняют 61,91% общей вариации.

### 1.3 Построение графика накопленного вклада главных компонент в суммарную дисперсию исходного признакового пространства.

Теперь построим график накопленного вклада г.к в суммарную дисперсию исходного признакового пространства.

```{r}
cumvarsum <- cumsum(princomp$sdev^2 / sum(princomp$sdev^2))*100
plot(cumvarsum, type = 'b', main = '', xlab = 'Номер компоненты', 
     ylab = 'Кумулятивное значение вариации, %')
abline(h = 70, col = 'red', lwd = 2)
abline(h = 80, col = 'blue', lwd = 2)
text(x = 13, y = 75, '70%', col = 'red')
text(x = 13, y = 85, '80%', col = 'blue')
```

По графику видно, что третья компонента сильно не дотягивает до установленного уровня в 70%.
Однако, в нашем случае было принято решение все же оставить три компоненты, поскольку при проверке двумя другими методами именно они были определены.
В ходе дальнейшего исследования так же было выявлено, что три компоненты являются более предпочтительными.

### 1.4 Интерпретация главных компонент на основе анализа матрицы факторных нагрузок. Дать названия выделенным главным компонентам.

Начнем построение матрицы факторных нагрузок с вращением Varimax:

```{r}
pca <- principal(df_s, nfactors = 3, rotate = "varimax")
print(pca$loadings, cutoff = 0.3)
```

Теперь проиинтерпретируем главные компоненты и дадим им название:

RC1.
Довольно интересный результат.
Получается, что либо из-за большой проходимости сотрудников не хватает, либо (что маловероятно) из-за числа сотрудников слишком маленькая проходимость — не всех же людей с улицы берут на работу.
Выходит, что в исследуемом заведении персонал не совсем оптимизирован, и его, скорее всего, не хватает, чтобы справляться с потоком.
То есть выгодное место используется неэффективно.
Название компоненты: Эффективность использования расположения (Location Utilization Efficiency)

RC3.
Показывает положительную связь между дневными затратами на маркетинг и количеством покупателей в день.
Довольно очевидно, что хорошая маркетинговая кампания приносит плоды: люди создают дополнительный спрос, что побуждает увеличивать рекламные расходы для повыщения прибыли.
Название компоненты: Отдача от маркетинга (Marketing ROI)

RC2.
Показывает связь между затратами на маркетинг и средним чеком покупки в кофейне.
Логично, что привлечение клиентов и количество заказов взаимосвязаны — похожая зависимость, как и в компоненте 3.
Название компоненты: Количество заказов, привлеченных рекламой (Advertising-Driven Order Volume)

## 2. Построение уравнения регрессии с использованием выделенных ГК

### 2.1. Построение линейного уравнения регрессии на ГК.

```{r}
pca <- as.data.frame(pc$ind$coord[, 1:3])  
pc1 <- pca$Dim.1  
pc2 <- pca$Dim.2  
pc3 <- pca$Dim.3 
```

```{r}
lm_pca <- lm(df$Daily_Revenue ~ pc1 + pc2 + pc3)  
summary(lm_pca)  
```

Общая F-статистика модели составляет 1132, при этом соответствующее значение p-value равно 2.2e-16, что значительно меньше любого принятого уровня значимости.
Это свидетельствует о статистической значимости уравнения линейной регрессии, а значит, модель в целом является информативной и полезной для анализа.

Коэффициент детерминации составляет 0.6298, что означает, что модель объясняет около 63% вариации зависимой переменной.

Уравнение линейной регрессии выглядит следующим образом: $y = 1917.33 + 307.11 * x_{1} + 425.75 * x_{2} + 556.89 * x_{3}$

### 2.2. Сопоставление свойств ранее полученных уравнений регрессии (линейное и нелинейное уравнения регрессии) с уравнением регрессии на ГК.

Изучим модель и определим информационные критерии Акаике (AIC) и Шварца (BIC):

```{r}
 IC_table <- data.frame(
 n = c('lm_pca'),
 a = c(AIC(lm_pca)),
 b = c(BIC(lm_pca))
 )
 kbl(IC_table, caption = "Таблица 1. Информационные критерии Акаике и Шварца",
 booktabs = TRUE, col.names = c("Модель", "Значение AIC", "Значение BIC")) %>%
 kable_classic(html_font = "Cambria", font_size = 12, full_width = FALSE)
```

Cоставим таблицу информационных критериев и степени объясняемости модели:

```{r}
 tab <- matrix (c('54%', 31652.0274, 31668.8302, '89%', 28779.9685, 28819.1748, '89%', 28778.3453, 28806.3498, '88%', -756.5574, -717.3546, '63%', 31232.15, 31260.15), ncol= 3 , byrow= TRUE )
 colnames(tab) <- c('R^2','Значение AIC', 'Значение BIC')
 rownames(tab) <- c('lm1','lm2','lm3', 'nls_model_all_vars', 'lm_pca')
 tab <- as.table (tab)
 pander(tab)
```

При сравнении моделей по информационным критериям AIC и BIC наименьшие значения наблюдаются у nls_model_all_vars (AIC = -756.5574, BIC = -717.3546), что указывает на её статистическую предпочтительность.
Однако, прямое сравнение линейных и нелинейных моделей по этим критериям некорректно из-за различий в спецификации.
Среди линейных моделей наилучшие показатели AIC и BIC у lm3 (AIC = 28778.3453, BIC = 28806.3498), а наибольшая объясняющая способность (R\^2 = 89%) — у 1m2 и 1m3.
Модель 1m_pca (R\^2 = 63%) демонстрирует более слабую интерпретацию данных, но её преимущество — сокращение числа переменных за счёт метода главных компонент (МГК), что упрощает анализ.

### 2.3. Выбор и обоснование лучшего уравнения.

На основании проведенного анализа можно сделать следующий вывод: лучшей моделью по информационным критериям AIC и BIC является степенная модель nls_model_all_vars, которая демонстрирует значительно более низкие значения этих критериев (AIC = -756.5574, BIC = -717.3546) по сравнению с линейными моделями.Это свидетельствует о ее статистической предпочтительности.

Однако важно учитывать, что прямое сравнение коэффициента детерминации R\^2 между степенной и линейными моделями некорректно, поскольку в степенной модели используется преобразованная зависимая переменная.

Среди линейных моделей наилучшие показатели демонстрирует модель lm3, имеющая высокий R\^2 (89%) и относительно низкие значения AIC (28778.3453) и BIC (28806.3498).

Модель с методом главных компонент (lm_pca) показывает более низкую объясняющую способность (R\^2 = 63%), но может быть полезна для работы с многомерными данными.

## 3. Кластерный анализ.

### 3.1. Построение и анализ дендрограмм

Отбор классифицирующих признаков:

```{r}
corrplot(cor(scale(df_s)), type = "full", method = "circle", tl.col = "black", tl.srt = 45, tl.cex = 0.5)
```

Как было сказано в начале работы, наши признаки не коррелированы между собой.

Возьмем выборку из 1000 наблюдений, так как на большем количестве данных сложно построить дендограмму:

```{r}
df_cluster <- df_s
df_cluster_short <- df_s[1:1000,]
```

Перед выполнением кластерного анализа необходимо стандартизировать признаки, поскольку это является обязательным шагом при использовании евклидовой метрики:

```{r}
cluster_data <- scale(df_cluster) 
cluster_data_short <- scale(df_cluster_short)
boxplot(cluster_data) 
```

Допустим, что число кластеров равно 2, и это значение будет впоследствии проверено и обосновано с помощью метода силуэта, метода локтя и анализа изменения статистики разрыва в зависимости от количества кластеров.
Используя факт того, что выборка однородна и подчиняется нормальному распределению, для методов, использующих дендрограммы, сократим выборку до 1000 наблюдений - основной целью является более быстрая работа алгоритма и графическое представление методов.

Метод Варда:

```{r}
hclust_w <- hcut(cluster_data_short, k = 2, hc_metric = 'euclidian', hc_method = 'ward.D2') 
```

Метод ближнего соседа:

```{r}
hclust_nn <- hcut(cluster_data_short, k = 2, hc_metric = 'euclidian', hc_method = 'single') 
```

Метод дальнего соседа:

```{r}
hclust_fn <- hcut(cluster_data_short, k = 2, hc_metric = 'euclidian', hc_method = 'complete') 
```

Метод средней связи:

```{r}
hclust_av <- hcut(cluster_data_short, k = 2, hc_metric = 'euclidian', hc_method = 'average') 
```

Метод центра тяжести:

```{r}
hclust_c <- hcut(cluster_data_short, k = 2, hc_metric = 'euclidian', hc_method = 'centroid') 
```

Демонстрация метода Варда:

```{r}
fviz_dend(hclust_w, 
cex = 1, 
color_labels_by_k = TRUE, 
main = 'Принцип Варда', ylab = 'Расстояние') 
```

Демонстрация метода ближнего соседа: Дендограмма не интерпретируема.

Демонстрация метода дальнего соседа:

```{r}
fviz_dend(hclust_fn, cex = 1, color_labels_by_k = TRUE, 
main = 'Принцип дальнего соседа', ylab = 'Расстояние') 
```

Демонстрация метода средней связи:

```{r}
fviz_dend(hclust_av, cex = 1, color_labels_by_k = TRUE, 
main = 'Принцип средней связи', ylab = 'Расстояние')
```

Демонстрация метода центра тяжести: Дендограмма не интерпретируема.

Как видно, метод Варда обеспечил наиболее удачное разбиение данных.

Далее определим оптимальное количество кластеров уже известными нам методами: методом "локтя" (анализ графика зависимости остаточной суммы квадратов отклонений от числа кластеров), методом силуэтов и применением Gap-статистики.

1)  Метод "локтя".

```{r}
fviz_nbclust(cluster_data, kmeans, method = 'wss') +
  labs(x = 'Число кластеров', y = 'Сумма внутрикластерных дисперсий',
       title = 'Зависимость WSS от числа кластеров')
```

Напомним, что оптимальное количество кластеров на графике соответствует значению (по оси абсцисс) точки, после которой отрицательный прирост значений функции начинает уменьшаться (значения перестают падать резко).
Из графика можно сделать вывод, что оптимальное число кластеров для исследования = 2: очень заметно сглаживание графика правее этой точки.

В качестве следующего шага применим метод силуэтов, чтобы уточнить результаты.

2)  Метод силуэтов.

```{r}
fviz_nbclust(cluster_data, kmeans, method = 'silhouette') +
  labs(x = 'Число кластеров', y = 'Средняя ширина силуэта по всем точкам',
       title = 'Зависимость средней ширины силуэта от числа кластеров')
```

Напомним, что силуэтом называется разница в числителе, а шириной - ее соотношение с максимумом.
При кластеризации хорошего качества среднее расстояние от объекта до соседей по кластеру не должно превышать минимального среднего расстояния по прочим кластерам.
Оптимальным следует считать значение параметра, соответствующее наибольшему среднему значению ширины силуэта.

Результат текущего теста расходится с результатом предыдущего: оптимальное число кластеров равняется 10, что достаточно много и плохо интерпретируемо для имеющейся выборки и количества независимых переменных.

3)  Gap-статистика. Дополнительно проверим GAP-статистику, проверяющую нулевую гипотезу о том, что выборка является однородной, то есть образует один целый кластер. В случае однородности выборки оптимальное число кластеров будет равно единице.

```{r}
fviz_nbclust(cluster_data, kmeans, method = 'gap_stat') +
  labs(x = 'Число кластеров', y = 'Статистика разрыва',
       title = 'Зависимость статистики разрыва от числа кластеров')
```

Нулевая гипотеза отвергается, что означает, что в рамках данного статистического критерия разделение на кластеры кофейн имеет смысл.

Gap-статистика указывает на то, что оптимальное число кластеров принимает значение, равное трем.

Дополнительно построим матрицу расстояний в евклидовом пространстве (для случайной выборки n = 30, так как при огромном количестве признаков данная матрица будет мало интерпретируемой):

```{r}
eucl_dist <- dist(cluster_data[sample(rownames(cluster_data), 30),], method = 'euclidean')
fviz_dist(eucl_dist)
```

## 4. Использование метода к-средних для классификации объектов.

Взглянем еще раз на корреляционную матрицу.

Можно заметить, что проблемы мультиколлинеарности не возникает (зависимые переменные очень слабо коррелирует между собой), следовательно, не имеет смысла удалять переменные перед переходом к евклидовой метрике и реализация метода k-средних.

Для полноты исследования проведем анализ, используя гипотезы о том, что оптимальное количество равно двум - кофейни все-же различаются на две группы.

Проведем кластеризацию данных по всей выборке с использованием метода k-means.

Чтобы избежать неустойчивости центров (в силу того, что центры кластеров каждый раз выбираются рандомно) допишем алгоритм повторной генерации (nstart = 30) и выведем устойчивое среднее значений.

Выберем оптимальное число центров кластеров = 2:

```{r}
kmeans_r <- kmeans(cluster_data, centers = 2, nstart = 30)
kmeans_r$centers[1,]
```

### 4.1.Построение и анализ графика средних значений показателей в кластерах.

Визуализируем результаты кластеризации для гипотезы через график средних.

По графику средних дадим интерпретацию полученным кластерам.

```{r}
cluster_colors <- c('blueviolet', 'firebrick3', 'orange', 'darkslategray4', 'darkslategray3')
par(mar = c(8, 4, 4, 2) + 0.1)  # Увеличиваем нижний отступ для подписей
plot(1:ncol(cluster_data), kmeans_r$centers[1,], 
     xaxt = 'n', 
     type = 'l', 
     col = cluster_colors[1], 
     lwd = 2, 
     ylim = c(-2, 5),
     ylab = 'Среднее значение признака', 
     xlab = '',
     main = 'График средних (признаки стандартизованы)')
for (i in 2:min(5, nrow(kmeans_r$centers))) {
  lines(1:ncol(cluster_data), kmeans_r$centers[i,], 
        type = 'l', 
        col = cluster_colors[i], 
        lwd = 2)
}
feature_names <- c('Число клиентов в день', 'Средний чек заказа', 'Количество сотрудников', 'Расходы на маркетинг в день', 'Пешеходный трафик в районе')
axis(1, at = 1:length(feature_names), labels = FALSE)
text(x = 1:length(feature_names), 
     y = par("usr")[3] - 0.5, 
     labels = feature_names,
     srt = 45,  # Наклон 45 градусов
     adj = 1,
     xpd = TRUE,
     cex = 0.8)
mtext("Определяющий критерий", side = 1, line = 6, cex = 0.8)
legend("topright", 
       legend = paste('Кластер', 1:min(5, nrow(kmeans_r$centers))), 
       lwd = 2, 
       col = cluster_colors[1:min(5, nrow(kmeans_r$centers))], 
       cex = 0.8)
```

Более детальный график кластеров:

```{r}
cluster_colors <- c('red', 'orange')
plot(1:ncol(cluster_data), kmeans_r$centers[1,], 
     type = 'n',  
     xaxt = 'n', 
     ylim = range(kmeans_r$centers) + c(-0.5, 0.5),  
     xlab = 'Признаки', 
     ylab = 'Стандартизованные средние значения',
     main = 'Средние значения признаков по кластерам',
     cex.lab = 0.9,
     las = 1)  
for (i in 1:2) {
  lines(1:ncol(cluster_data), kmeans_r$centers[i,], 
        type = 'l', 
        col = cluster_colors[i], 
        lwd = 2)
}
axis(1, at = 1:ncol(cluster_data), labels = FALSE)  
text(x = 1:ncol(cluster_data), 
     y = par("usr")[3] - 0.2,  
     labels = names(cluster_data), 
     srt = 45,  
     adj = 1,   
     xpd = TRUE,  
     cex = 0.7)  
legend("topright", 
       legend = paste("Кластер", 1:2),
       col = cluster_colors,
       lwd = 2,
       cex = 0.8,
       bty = "n") 
grid(nx = NA, ny = NULL, col = "lightgray", lty = "dotted")
```

### 4.2. Интерпретация полученных кластеров.

График, построенный на основе гипотезы о том, что оптимальное число кластеров соответствует двум, дает представление о ситуации на рынке кофеин, взятых по выборке.

Кластер 1: кофейни, вкладывающиеся в маркетинг меньше среднего уровня, расширяющие приближенность к новым районам в большей степени и имеющие низкий поток клиентов - вероятно, развивающиеся кофейни, которые еще не достигли потенциальных значений выручки и популярности.

Кластер 2: уже сложившиеся кофейни, осознающие важность маркетинга для оптимизации продаж, имеющие большой поток клиентов и расширяющие приближенность к новым районам в меньшей степени.

### 4.3. Проверка гипотезы о равенстве средних значений в кластерах.

Дополнительно проведем проверку равенства средних для более точного разделения обьнктов выборки на кластеры.

Создадим две выборки по нашим полученным кластерам:

```{r}
f_1 <- cluster_data[names(kmeans_r$cluster[kmeans_r$cluster == 1]),]
f_2 <- cluster_data[names(kmeans_r$cluster[kmeans_r$cluster == 2]),]
```

```{r}
df_f1 <- data.frame(f_1)
print(df_f1)
df_f2 <- data.frame(f_2)
```

```{r}
df_n1 <- rbind(df_f1, df_f2)
head(df_n1)
```

```{r}
str(df_n1)
```

```{r}
features <- c("Number_of_Customers_Per_Day", "Average_Order_Value", 
             "Number_of_Employees", "Marketing_Spend_Per_Day", 
             "Location_Foot_Traffic")
anova_results <- list()
for (feature in features) {
  # Создаем объединенный датафрейм с меткой кластера
  df_f1$Cluster <- "Cluster1"
  df_f2$Cluster <- "Cluster2"
  combined_df <- rbind(df_f1[, c(feature, "Cluster")], 
                      df_f2[, c(feature, "Cluster")])
  anova_model <- aov(as.formula(paste(feature, "~ Cluster")), data = combined_df)
  anova_results[[feature]] <- summary(anova_model)
}
for (feature in features) {
  cat("\n--- ANOVA для признака:", feature, "---\n")
  print(anova_results[[feature]])
}
anova_table <- do.call(rbind, lapply(features, function(feature) {
  df_f1$Cluster <- "Cluster1"
  df_f2$Cluster <- "Cluster2"
  combined_df <- rbind(df_f1[, c(feature, "Cluster")], 
                      df_f2[, c(feature, "Cluster")])
  
  tidy(aov(as.formula(paste(feature, "~ Cluster")), data = combined_df)) |>
    mutate(Feature = feature)
}))
anova_table <- anova_table[anova_table$term == "Cluster", ]
print(anova_table[, c("Feature", "df", "sumsq", "meansq", "statistic", "p.value")])
```

```{r}
model1 <- aov(Number_of_Customers_Per_Day ~ Daily_Revenue, data = df)
summary(model1)
model2 <- aov(Average_Order_Value ~ Daily_Revenue, data = df)
summary(model2)
model3 <- aov(Number_of_Employees ~ Daily_Revenue, data = df)
summary(model3)
model4 <- aov(Marketing_Spend_Per_Day ~ Daily_Revenue, data = df)
summary(model4)
model5 <- aov(Location_Foot_Traffic ~ Daily_Revenue, data = df)
summary(model5)
```

По результатам ANOVA делаем выводы: 

1. Средние для критериев "Количество посетителей в день", "Средний чек заказа" и "Расходы на маркетинг в день" сильно отличаются друг от друга на уровне значимости p = 0, "Пешеходный трафик в районе" отличаются на уровне значимости p = 0,001 - средние по каждому из критериев значимо отличаются друг от друга.

2. Для критериев "Пешеходный трафик в районе" и "Количество сотрудников" отклонения средних незначимо.

### 4.4. Описание кластеров с помощью графических средств, помогающих обосновать название кластеров.

Построим графическую интерпретацию кластеров:

1)  Параллельные координаты.

```{r}
combined_data <- rbind(
  cbind(df_f1, Source = "Sample_1"),
  cbind(df_f2, Source = "Sample_2")
)
combined_data$Cluster <- as.factor(kmeans_r$cluster)
ggparcoord(combined_data,
           columns = 1:(ncol(combined_data)-2),  
           groupColumn = "Cluster",
           alphaLines = 0.4,
           scale = "uniminmax",
           title = "Parallel Coordinates: Clusters from k-means") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5),
        legend.position = "bottom") +
  scale_color_brewer(palette = "Set1") +
  facet_grid(. ~ Source)  
important_vars <- c("Var1", "Var3", "Var5")  
```

С учетом того, что выборка состоит из 2000 элементов, сложно увидеть точную картину кластеризации.
Построим графики fviz:

```{r}
fviz_cluster(object = kmeans_r,data = cluster_data, ellipse.type = 'convex')
```

```{r}
fviz_cluster(object = kmeans_r,data = cluster_data, choose.vars = c('Number_of_Customers_Per_Day', 'Average_Order_Value'), ellipse.type = 'convex', geom = 'point')
```

```{r}
fviz_cluster(object = kmeans_r,data = cluster_data, choose.vars = c('Marketing_Spend_Per_Day', 'Average_Order_Value'), ellipse.type = 'convex', geom = 'point')
```

На обоих графиках представлены два исходных кластера, где красный - сложившиеся кофейни с постоянным высоким потоком клиентов, синий - развивающиеся кофейни.

Однако присутствует интересная деталь: смещение середины происходит вверх для графика с числом покупателем, обратная ситуация на графике с расходами на маркетинг.

Приводит к первому выводу, что для развивающихся кофеин имеет смысл привлекать больше клиентов, что приведет к увеличению среднего чека.

Второй вывод не настолько однозначен: для развивающихся кофеин при несоразмерном повышении расходов на маркетинг средний чек может уменьшиться (слишком агрессивная реклама или недостаточные капиталовложения в фундаментальные точки роста бизнеса).

### 4.5. Выводы.

С учетом гипотезы о том, что имеется два кластера, кофейни были разделены на условные группы: Группа 1: Развивающиеся кофейни (синяя группа) (кофейни, вкладывающиеся в маркетинг меньше среднего уровня, расширяющие приближенность к новым районам в большей степени и имеющие низкий поток клиентов) Группа 2: Сложившиеся кофейни (красная группа), осознающие важность маркетинга для оптимизации (имеющие большой поток клиентов и расширяющие приближенность к новым районам в меньшей степени)

### 5. Построение регрессионных моделей в кластерах (типологическая регрессия).

Применим метод k-средних для построения модели регрессии (включим зависимую переменную):

```{r}
set.seed(129)
kmeans_r <- kmeans(df, centers = 2, nstart = 30)
df$cluster <- kmeans_r$cluster
```

```{r}
data_cluster1 <- df[df$cluster == 1, ]
data_cluster2 <- df[df$cluster == 2, ]
```

Получим регрессионную модель для обоих кластеров.

Для кластера 1:

```{r}
lm_cluster1 <- lm(Daily_Revenue ~ Number_of_Customers_Per_Day + Average_Order_Value + 
                  Number_of_Employees + Marketing_Spend_Per_Day + Location_Foot_Traffic, 
                  data = data_cluster1)
summary_cluster1 <- summary(lm_cluster1)
```

Для кластера 2:

```{r}
lm_cluster2 <- lm(Daily_Revenue ~ Number_of_Customers_Per_Day + Average_Order_Value + 
                  Number_of_Employees + Marketing_Spend_Per_Day + Location_Foot_Traffic, 
                  data = data_cluster2)
summary_cluster2 <- summary(lm_cluster2)
```

И выведем все характеристики моделей.

Для первого кластера:

```{r}
cat("Cluster 1 Regression Coefficients:\n")
coef(lm_cluster1)
cat("\nCluster 1 Summary:\n")
summary_cluster1
```

И для второго кластера:

```{r}
cat("\nCluster 2 Regression Coefficients:\n")
coef(lm_cluster2)
cat("\nCluster 2 Summary:\n")
summary_cluster2
```

Для оценки качества регрессионных моделей, построенных для двух кластеров и для всего набора данных в целом, сравним их по следующим метрикам: R-квадрат (R²), скорректированный R-квадрат, AIC (информационный критерий Акаике), BIC (байесовский информационный критерий) и среднеквадратичная ошибка (MSE).

Для кластера 1:

```{r}
r2_cluster1 <- summary_cluster1$r.squared
adj_r2_cluster1 <- summary_cluster1$adj.r.squared
aic_cluster1 <- AIC(lm_cluster1)
bic_cluster1 <- BIC(lm_cluster1)
mse_cluster1 <- mean(resid(lm_cluster1)^2)
```

Для кластера 2:

```{r}
r2_cluster2 <- summary_cluster2$r.squared
adj_r2_cluster2 <- summary_cluster2$adj.r.squared
aic_cluster2 <- AIC(lm_cluster2)
bic_cluster2 <- BIC(lm_cluster2)
mse_cluster2 <- mean(resid(lm_cluster2)^2)
```

Для всего набора данных:

```{r}
lm_all <- lm(Daily_Revenue ~ Number_of_Customers_Per_Day + Average_Order_Value + 
             Number_of_Employees + Marketing_Spend_Per_Day + Location_Foot_Traffic, 
             data = df)
summary_all <- summary(lm_all)
r2_all <- summary_all$r.squared
adj_r2_all <- summary_all$adj.r.squared
aic_all <- AIC(lm_all)
bic_all <- BIC(lm_all)
mse_all <- mean(resid(lm_all)^2)
```

Вывод результатов:

```{r}
cat("Cluster 1: R² =", r2_cluster1, ", Adj R² =", adj_r2_cluster1, ", AIC =", aic_cluster1, ", BIC =", bic_cluster1, ", MSE =", mse_cluster1, "\n")
cat("Cluster 2: R² =", r2_cluster2, ", Adj R² =", adj_r2_cluster2, ", AIC =", aic_cluster2, ", BIC =", bic_cluster2, ", MSE =", mse_cluster2, "\n")
cat("All Data: R² =", r2_all, ", Adj R² =", adj_r2_all, ", AIC =", aic_all, ", BIC =", bic_all, ", MSE =", mse_all, "\n")
```

4.  Выводы. R² и скорректированный R² для моделей кластеров выше, чем для общей модели - это означает, что кластеризация улучшает объяснительную способность моделей и она имела смысл. Меньшие значения AIC и BIC для моделей кластеров указывают на их предпочтительность с учетом сложности. Меньшие значения MSE для моделей кластеров свидетельствуют о более точных предсказаниях по сравнению с общей моделью. На основе сравнения метрик можно сделать следующие выводы: В обеих моделях на уровне значимости p = 0 характеристики количества работников, среднего чека покупки в кофейне и ежедневных расходов на маркетинг статистически значимы, что подтверждает факты, полученные в предыдущих пунктах. Стоит также отметить, что переменная Intercept, отвечающая за прочие факторы, не вошедшие в модель, так же имеет сильное влияние на зависимую переменную. Исправленнын коэффициенты детерминации R\^2 для обоих кластеров высоки (0.7489 и 0,89), что соответствует объясняющей способности модели - факторы обьясняют подавляющую долю дисперсии (75% и 89% соответственно). R\^2 для всей выборки - 0,89, что также является хорошим результатом для модели. Если метрики (R², скорректированный R², AIC, BIC, MSE) для моделей кластеров превосходят соответствующие значения для общей модели, это подтверждает, что разделение данных на кластеры (например, развивающиеся и устоявшиеся кофейни) позволяет строить более точные и интерпретируемые регрессионные модели. Кластеризация учитывает гетерогенность данных, что улучшает подгонку моделей.

Анализ коэффициентов регрессии (coef(lm_cluster1) и coef(lm_cluster2)) показывает, как факторы влияют на доход в каждом кластере: Можно сформировать рекомендации для обеих выборок: Для кластера 1: стоит инвестировать в маркетинг и выбор местоположения с высоким трафиком.

Для кластера 2: сосредоточиться на удержании клиентов и увеличении среднего чека.

```{r}
cl <- kmeans_r$cluster
cluster_data_lda <- as.data.frame(cbind(cluster_data, cl))
```

Разделяем данные на обучающую (2/3) и тестовую (1/3) выборки:

```{r}
smpl_size <- floor(2/3 * nrow(cluster_data_lda))
set.seed(123)
train_ind <- sample(seq_len(nrow(cluster_data_lda)), size = smpl_size)

data.train <- as.data.frame(cluster_data_lda[train_ind, ])
data.unknown <- as.data.frame(cluster_data_lda[-train_ind, ])
```

## 6. Линейный дискриминантный анализ (LDA).

### 6.1. Построение дискриминантных функций. Выводы о качестве модели.

Строим LDA модель (дискриминантную функцию):

```{r}
lda.fit <- lda(cl ~ Number_of_Customers_Per_Day + Average_Order_Value + Number_of_Employees + Marketing_Spend_Per_Day + Location_Foot_Traffic, data = data.train)
lda.fit
```

На данном этапе мы построили дискриминантную функцию, благодаря которой сможем увидеть коэффициенты линейной дискриминации, которые говорят о том, что дискриминантная функция является линейной комбинацией параметров со значениями: DF = 0.0735 \* Number_of_Customers_Per_Day + 0.1296 \* Average_Order_Value + 0.2596 \* Number_of_Employees + 1.9112 \* Marketing_Spend_Per_Day + 0.15097 \* Location_Foot_Traffic

Оценка и выводы качества модели:

```{r}
lda.pred <- predict(lda.fit, data.unknown[, c("Number_of_Customers_Per_Day", "Average_Order_Value", "Number_of_Employees", "Marketing_Spend_Per_Day",             "Location_Foot_Traffic")])
names(lda.pred)
```

Оцениваем качество с помощью лямбды Уикса:

```{r}
ldam <- manova(as.matrix(data.unknown[, c("Number_of_Customers_Per_Day", "Average_Order_Value", "Number_of_Employees", "Marketing_Spend_Per_Day",  "Location_Foot_Traffic")]) ~ lda.pred$class)
summary(ldam, test = "Wilks")
```

Модель демонстрирует высокое качество, так как низкое значение лямбда Уилкса 0.2426, но при этом больше 0.05 и крайне малое p-value \< 2.2e-16 указывают на существенные различия между средними значениями кластеров.

Дискриминантная функция, построенная на основе предикторов, эффективно разделяет кофейни на два кластера, что подтверждает пригодность модели для классификации.

### 6.2. Отнесение новых объектов к выделенным и описанным кластерам различными способами с использованием ДФ.

Наблюдение с низкими значениями (ожидается 2-й кластер)

```{r}
predict(lda.fit, newdata = data.frame(Number_of_Customers_Per_Day = 150,  Average_Order_Value = 4.0, Number_of_Employees = 4, Marketing_Spend_Per_Day = 100.0,  Location_Foot_Traffic = 250       
))
```

Наблюдение с высокими значениями (ожидается 1-й кластер):

```{r}
predict(lda.fit, newdata = data.frame(Number_of_Customers_Per_Day = 400,Average_Order_Value = 9.0,Number_of_Employees = 11,Marketing_Spend_Per_Day = 400.0,    
Location_Foot_Traffic = 900       
))
```

Еще одно наблюдение с высокими значениями (ожидается 1-й кластер)

```{r}
print(predict(lda.fit, newdata = data.frame(Number_of_Customers_Per_Day = 450, Average_Order_Value = 8.5, Number_of_Employees = 12, Marketing_Spend_Per_Day = 350.0,   
Location_Foot_Traffic = 850       
)))
```

Модель успешно классифицирует новые наблюдения, отображая низкие значения во 2-й кластер и высокие в 1-й.

## 6.3. Уточнение результатов классификации, выполненной с помощью метода к-средних, с помощью аппарата дискриминантного анализа (выявление некорректно классифицированных наблюдений).

Гистограмма значений дискриминантной функции:

```{r}
plot(lda.fit, main = "Гистограмма значений дискриминантной функции")
```

Гистограммы подтверждают, как отличаются значения двух групп, что говорит о том, что есть четкая кластеризация.

Диаграмма рассеяния для дискриминантных функций:

```{r}
plot(lda.pred$x[, 1], col = data.unknown$cl, pch = 19, main = "Диаграмма рассеяния (LDA)")
text(lda.pred$x[, 1], labels = data.unknown$cl, cex = 0.7, pos = 4, col = "blue")
```

Диаграмма рассеяния предназначена для отображения схожести значений между дискриминантными функциями.

Однако, поскольку в нашем анализе используется только одна дискриминантная функция, данная диаграмма не позволяет сделать содержательные выводы.

Таблица соответствия предсказанных и исходных классов:

```{r}
table(lda.pred$class, data.unknown$cl, dnn = c("Predicted", "Actual"))

summary(lda.pred$class)
```

Класс 1 - Из 322 фактических наблюдений класса 1 модель правильно классифицировала 321 наблюдение , но ошибочно отнесла 1 наблюдение к классу 2.

Класс 2 - Из 345 фактических наблюдений класса 2 модель правильно классифицировала 342 наблюдения, но ошибочно отнесла 3 наблюдения к классу 1.

## 6.4. Анализ классификационной матрицы (classification matrix). Вывод о качестве разбиения объектов на кластеры.

```{r}
misclass <- function(pred, obs) {tbl <- table(pred, obs)
sum <- colSums(tbl)
dia <- diag(tbl)
msc <- ((sum - dia)/sum) * 100
m.m <- mean(msc)
cat("Classification table:", "\n")
print(tbl)
cat("Misclassification errors:", "\n")
print(round(msc, 2))

print(round(m.m, 2))}

misclass(lda.pred$class, data.unknown[,c("cl")])
```

Модель линейного дискриминантного анализа демонстрирует высокую точность классификации — 99.4% (663 из 667 наблюдений).Класс 1 правильно классифицировано 321 из 324 наблюдений, класс 2 — 342 из 343.
Ошибки классификации минимальны (0.6%): 3 кофейни из класса 1 ошибочно отнесены к классу 2, 1 из класса 2 — к классу 1.
Метрики ошибок (0.93 для класса 1 и 0.29 для класса 2) подтверждают высокую предсказательную способность.
Доля объясненной дисперсии (0.61) указывает на умеренную способность модели разделять кластеры.

## 6.5. Построение графика принадлежности тестовой и тренировочной выборок к кластерам по результатам проведенного анализа.

Метод Варда, ранее использованный при проведении кластерного анализа, также применим для визуализации принадлежности объектов тренировочной и тестовой выборок к кластерам, полученным в ходе дискриминантного анализа.
Поскольку для обучения модели было использовано 2/3 всей выборки, именно это количество наблюдений и будет использоваться при построении соответствующего графика.

```{r}
data.train_new <- data.train[1:min(333, nrow(data.train)), 
                             c("Number_of_Customers_Per_Day", "Average_Order_Value", 
                               "Number_of_Employees", "Marketing_Spend_Per_Day", 
                               "Location_Foot_Traffic")]
hclust_w1 <- hcut(data.train_new, k = 2, hc_metric = "euclidean", hc_method = "ward.D2")
fviz_dend(hclust_w1, cex = 1, color_labels_by_k = TRUE, 
          main = "Дендрограмма (принцип Варда) ", ylab = "Расстояние")
```

Тестовая выборка (укажем 1/3 наблюдений от выборки):

```{r}
data.unknown_new <- data.unknown[1:min(167, nrow(data.unknown)), 
                                 c("Number_of_Customers_Per_Day", "Average_Order_Value", 
                                   "Number_of_Employees", "Marketing_Spend_Per_Day", 
                                   "Location_Foot_Traffic")]
hclust_w2 <- hcut(data.unknown_new, k = 2, hc_metric = "euclidean", hc_method = "ward.D2")
fviz_dend(hclust_w2, cex = 1, color_labels_by_k = TRUE, 
          main = "Дендрограмма (принцип Варда) (тестовая выборка)", ylab = "Расстояние")
```

Дендрограмма, построенная для тестовой выборки, по своей структуре схожа с дендрограммой обучающей выборки, хотя визуально она выглядит более крупной, что обусловлено меньшим количеством наблюдений.
Обе дендрограммы демонстрируют чёткое и логичное разделение данных на два кластера.

Для оценки качества регрессионных моделей, построенных для двух кластеров и для всего набора данных в целом, сравним их по следующим метрикам: R-квадрат (R²), скорректированный R-квадрат, AIC (информационный критерий Акаике), BIC (байесовский информационный критерий) и среднеквадратичная ошибка (MSE).

# Для кластера 1

```{r}
r2_cluster1 <- summary_cluster1$r.squared
adj_r2_cluster1 <- summary_cluster1$adj.r.squared
aic_cluster1 <- AIC(lm_cluster1)
bic_cluster1 <- BIC(lm_cluster1)
mse_cluster1 <- mean(resid(lm_cluster1)^2)
```

# Для кластера 2

```{r}
r2_cluster2 <- summary_cluster2$r.squared
adj_r2_cluster2 <- summary_cluster2$adj.r.squared
aic_cluster2 <- AIC(lm_cluster2)
bic_cluster2 <- BIC(lm_cluster2)
mse_cluster2 <- mean(resid(lm_cluster2)^2)
```

# Для всего набора данных

```{r}
lm_all <- lm(Daily_Revenue ~ Number_of_Customers_Per_Day + Average_Order_Value + 
             Number_of_Employees + Marketing_Spend_Per_Day + Location_Foot_Traffic, 
             data = df)

summary_all <- summary(lm_all)
r2_all <- summary_all$r.squared
adj_r2_all <- summary_all$adj.r.squared
aic_all <- AIC(lm_all)
bic_all <- BIC(lm_all)
mse_all <- mean(resid(lm_all)^2)
```

# Вывод результатов

```{r}
cat("Cluster 1: R² =", r2_cluster1, ", Adj R² =", adj_r2_cluster1, ", AIC =", aic_cluster1, ", BIC =", bic_cluster1, ", MSE =", mse_cluster1, "\n")
cat("Cluster 2: R² =", r2_cluster2, ", Adj R² =", adj_r2_cluster2, ", AIC =", aic_cluster2, ", BIC =", bic_cluster2, ", MSE =", mse_cluster2, "\n")
cat("All Data: R² =", r2_all, ", Adj R² =", adj_r2_all, ", AIC =", aic_all, ", BIC =", bic_all, ", MSE =", mse_all, "\n")
```

Интерпретация

Если R² и скорректированный R² для моделей кластеров выше, чем для общей модели, это означает, что кластеризация улучшает объяснительную способность моделей.

Меньшие значения AIC и BIC для моделей кластеров указывают на их предпочтительность с учетом сложности.

Меньшие значения MSE для моделей кластеров свидетельствуют о более точных предсказаниях по сравнению с общей моделью.

4.Выводы На основе сравнения метрик можно сделать следующие выводы:

Улучшение качества моделей:

Если метрики (R², скорректированный R², AIC, BIC, MSE) для моделей кластеров превосходят соответствующие значения для общей модели, это подтверждает, что разделение данных на кластеры (например, развивающиеся и устоявшиеся кофейни) позволяет строить более точные и интерпретируемые регрессионные модели.
Кластеризация учитывает гетерогенность данных, что улучшает подгонку моделей.

Различия в зависимостях: Анализ коэффициентов регрессии (coef(lm_cluster1) и coef(lm_cluster2)) показывает, как факторы влияют на доход в каждом кластере.
Например:

В кластере 1 (возможно, развивающиеся кофейни) такие переменные, как Marketing_Spend_Per_Day и Location_Foot_Traffic, могут иметь большее влияние.

В кластере 2 (возможно, устоявшиеся кофейни) ключевыми факторами могут быть Number_of_Customers_Per_Day и Average_Order_Value.

Практическая значимость:

Для кластера 1: стоит инвестировать в маркетинг и выбор местоположения с высоким трафиком.

Для кластера 2: сосредоточиться на удержании клиентов и увеличении среднего чека.

Таким образом, кластеризация не только повышает качество предсказаний, но и предоставляет ценные инсайты для разработки стратегий, адаптированных к различным группам кофейных магазинов.
